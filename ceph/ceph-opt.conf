[global]
fsid = a7f64266-0894-4f1e-a635-d0aeaca0e993
mon initial members = mon1
mon host = 172.16.8.241
public network = 172.16.8.0/24
cluster network = 172.16.8.0/24
auth cluster required = cephx
auth service required = cephx
auth client required = cephx
osd pool default size = 3
osd pool default min size = 1
max open files = 131072
debug bluestore = 0/0
debug bluefs = 0/0
debug bdev = 0/0
debug rocksdb = 0/0
# spread objects over 8 collections
osd pool default pg num = 256
# increasing shards can help when scaling number of collections
#osd op num shards = 5
[client]
rbd cache = true
rbd cache size = 268435456
rbd cache max dirty = 134217728
rbd cache max dirty age = 5
rbd_default_features = 3
[mon]
mon data = /var/lib/ceph/mon/ceph-$id
[osd]
enable experimental unrecoverable data corrupting features = bluestore rocksdb
bluestore fsck on mount = true
bluestore block db size = 67108864
bluestore block wal size = 134217728
osd objectstore = bluestore
bluestore = true
osd data = /var/lib/ceph/osd/ceph-$id
osd mkfs type = xfs
osd mkfs options xfs = -f
osd max write size = 512
osd client message size cap = 2147483648
osd deep scrub stride = 131072
osd op threads = 8
osd disk threads = 4
osd map cache size = 1024
osd map cache bl size = 128
osd mount options xfs = “rw,noexec,nodev,noatime,nodiratime,nobarrier”
osd recovery op priority = 4
osd recovery max active = 10
osd max backfills = 4
